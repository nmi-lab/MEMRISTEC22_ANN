<section>
        <section data-markdown data-vertical-align-top data-background-color=#B2BA67><textarea data-template>
            <h1> Lecture 11: Neural Networks for Natural Language Processing <br/> </h1>

        </textarea></section>

      <section data-markdown><textarea data-template>
      <h2> Example Tasks</h2>                                                     
      <img src="images/rnn_tasks.jpeg" />
      <ul>
        <li class="fragment"/> Today: Sequence-to-sequence models for text-like data
      </ul>
      </textarea></section>

      <section data-markdown><textarea data-template>
      <h2> Word embeddings</h2>                                                     
      <ul>
        <li /> Words can be represented as an <i>integer</i>, one for each word in a vocabularny
<pre>Let's represent this sequence 
23 450 27 124</pre>
        <li /> The vocabulary size is the number of words we can represent. Typical sizes are 30000
        <li class=fragment /> A word is like one category among 30000 possibilities: it is very sparse and not practical to work in it.
          <li class=fragment /> Word embeddings are mappings that map a word into a real vector space of of smaller dimension (typicaly $\mathbb{R}^{256}$). 
<pre>Let's represent this sequence 
0.3  1.3   0.1  1.3
-0.5 2.3   5.5  -2.5
...  ...   ...   ...
1.6  0.0   -3.2  8.2
</pre>
        <li class=fragment /> Various techniques exist to learn this function, let's assume this mapping exists
      </ul>
      </textarea></section>

        <section data-markdown><textarea data-template>
        <h2> Feed-Forward vs. Recurrent Networks</h2>                                                     
        <ul>
          <li/> Due to the difficulties of training recurrent neural networks, they are now falling out of favor. 
            <ul>
              <li/> Sequential computations are difficult to parallelize
              <li/> No explicit modeling of long and short range dependencies (although Attention helps)
            </ul>
          <li/> This applies beyond NLP: State-of-the-art audio are also feed-forward networks (e.g. Wavenets)
        </ul>
        </textarea></section>

        <section data-markdown><textarea data-template>
        <h2> Feed-Forward Example For Time-Dependent Data: WaveNet</h2>
        <ul>
          <li/> Wavenet is a type of feedforward Convnet. It uses convolutions "Ã  trous" to obtain large receptive fields
        <div class=row>
          <div class=column>
            <img src="images/dilation.gif" />
          </div>
          <div class=column>
            <img src="images/dilated_wavenet.png" />
          </div>
        </div>
        <p class=ref> Van der Oord et al. 2016</p>
          <li/> Learns generative model: 
            $$
            p(\mathbf{x}) = \prod_t p(x_t|x_{t-1}, ..., x_0)
            $$
            <li/> Wavenets are the state-of-the-art in audio generation. See <a href="https://deepmind.com/blog/article/wavenet-generative-model-raw-audio"> Google's blog </a>.
        </ul>
        </textarea></section>

        <section data-markdown><textarea data-template>
        <h2> Feed-Forward Example For Language Understanding: BERT </h2>
        <ul>
          <li/> BERT is a popular language modeling architecture. It uses a feedforward "transformers with "self-attention block" to learn relations across text
        <div class=row>
          <div class=column>
            <img src="images/self-attention-block.png" class=medium />
          </div>
          <div class=column>
            <img src="images/bert.png" class=medium />
          </div>
        </div>
        <p class=ref> Vaswani et al. 2017, Devlin et al. 2017</p>
          <li/> Trained to predict missing words: ("My dog is [MASK]", predict target "Hairy") and next sentence prediction
        <div class=row>
          <div class=column>
        <blockquote>
        the man went to [MASK] store [SEP] he bought a gallon [MASK] milk 

        Label = IsNext
        </blockquote>
          </div>
          <div class=column>
        <blockquote>
        the man [MASK] to the store [SEP] penguin [MASK] are flight ##less birds 

        Label = NotNext
        </blockquote>
          </div>
        </div>
        </ul>
        </textarea></section>

        <section data-markdown><textarea data-template>
        <h2> Further Reading </h2>
        <ul>
          <li> <a href=http://web.stanford.edu/class/cs224n/slides/cs224n-2020-lecture01-wordvecs1.pdf > Word2Vec </a> </li>
          <li> <a href = http://nlp.seas.harvard.edu/2018/04/03/attention.html>Transformer</a></li>
          <li><a href=http://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture14-transformers.pdf>Self-Attention</a></li>
          <li><a href=http://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture08-nmt.pdf>Machine Translation</a></li>
        </ul>
        </textarea></section>

</section>
