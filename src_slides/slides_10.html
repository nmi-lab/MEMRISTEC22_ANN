<section>
        <section data-markdown data-vertical-align-top data-background-color=#B2BA67><textarea data-template>
            <h1> Lecture 10: Spiking Neural Networks <br/> </h1>

        </textarea></section>

        <section data-markdown><textarea data-template>
          <h2>Anatomy of the Neuron</h2>
          <img src="images/neuron_drawing.png" />

          - Dendrites: act as inputs ports 
          - Soma: the body of the cell, usually where inputs converge and where action potentials are generated 
          - Axon: propagates action potentials along to other neurons
          - Terminal Boutons (Synapses): act as outputs of the neuron
        </textarea></section>

        <section data-markdown><textarea data-template>
          <h2>Membrane potential</h2>
          <img src="images/bear-03-11.png" class=stretch />
        </textarea></section>

        <section data-markdown><textarea data-template>
          <h2>Action Potentials and the Axon</h2>
          <img src="images/neuron_drawing.png" />
          <img src="images/bear-04-02-1.png" />

          <p class=pl> Neurons communicate by all-or-none events called Action Potentials, or ``Spikes''</p>
        </textarea></section>

        <section data-markdown><textarea data-template>
        <h2> "Biological" neuron model: The Leaky Integrate and Fire Neuron. </h2>                                                     
        <div class=row>
        <div class=column>
        <ul>
          <li/> Membrane Voltage
            $$
            \begin{split}
            U_i(t) = &  V_i(t),\\
            \tau_{mem}\frac{\mathrm{d}}{\mathrm{d}t} V_i(t) = & - V_i(t) + I_i(t),\\
            \end{split}
            $$

          <li class=fragment /> Output Spike
            $$
            S_i = \Theta(U_i)
            $$

          <li class=fragment /> Synaptic Currents
            $$
            \begin{split}
             I_{i}(t) = \sum_{j\in \text{pre}} W_{ij} S_j(t),
            \end{split}
            $$
        </ul>
        </div>
        <div class=column>
          <img src="images/leaky_if.png" />
        </div>
        </textarea></section>


        <section data-markdown><textarea data-template>
        <h2> "Biological" neuron model: The Leaky Integrate and Fire Neuron. </h2>                                                     

          <img src="images/leaky_if.png"  class=stretch />
        </textarea></section>

        <section data-markdown><textarea data-template>
        <h2> Recurrent Neural Networks and Working Memory </h2>

        <div class=row>
        <div class=column>
        <p>Working Memory:</p>
        <ul>
        <li /> A type of short-term memory
        <li /> Limited in capacity
        <li /> Task- and sensory modality-dependent
        <li /> Necessary for cognitive control
        </ul>
        </div>
        <div class=column>
          <img src="images/brain_wm.png" />
          Human brain areas for working
        memory of face identity and
        location
        </div>
        </div>

        </textarea></section>

        <section data-markdown><textarea data-template>
        <h2> Neural Correlates of Working Memory </h2>

        <img src="images/primate_task.png" class=stretch />
        </textarea></section>

        <section data-markdown><textarea data-template>
        <h2> Working Memory of Cognitive Control </h2>

        <img src="images/12ax.png" class=stretch />
        <p class=ref>Oâ€™Reilly and Frank, 2006</p>

        <p class=pl> How does the brain implement working memory? </p>
        </textarea></section>

        <section data-markdown><textarea data-template>
        <h2> Recurrent Neural Networks (RNNs) in Neuroscience </h2>

        <ul>
          <li > Most models hypothesize that short term memory is a process supported by recurrent connections </li>
          <li > An RNN is a network in which the output feeds back into the network (A: Feedforward, B: Recurrent) </li> 
          <img src="images/feedfoward_vs_recurrent.png" class=small  />

        <li class=fragment > The majority of connections in the brain are recurrent
          <div class=row>
            <div class=column>
            <img src="images/cortical_microcircuit.png" />
            <p class=ref> Douglas and Martin, 1989</p>
            </div>
            <div class=column>
              <blockquote>
                ... physically mapped the synapses on the dendritic trees (...) in layer 4 of the cat primary visual cortex and found that only 5% of the excitatory synapses arose from the lateral geniculate nucleus (LGN)
              </blockquote>
              <p class=ref> Binzegger et al. 2004</p>
            </div>
          </div>
        </li>
        </ul>
        </textarea></section>

        <section data-markdown><textarea data-template>
        <h2> Recurrent Neural Networks (RNNs) in Neuroscience </h2>

        <ul>
          <li /> Recurrent connectivity can support sustained activity
          <div class=row>
          <div class=column>
          <img src="images/F1.large.jpg"  />
          </div>
          <div class=column>
          <img src="images/F2.large.jpg" class=large />
          </div>
          </div>
          <p class=ref> Murray et al. 2017 </p>
          <li class=fragment /> In the brain, how do such neural networks learn?
        </ul>

        </textarea></section>


<section data-markdown><textarea data-template>
  <h2>Types of Synaptic Plasticity in the Brain</h2>
  <div class=row>
    <div class=column >
    <center>Long-Term Plasticity</center>
    </div>
    <div class=column >
    <center>Short-Term Plasticity</center>
    </div>
  </div>

  <div class=row>
    <div class=column >
      <img src="images/ltp.png" />
    </div>
    <div class=column >
      <img src="images/stp.jpg" />
    </div>
    <p class=ref>Tsodyks_Markram97_neur-code</p>
  </div class=row>

  <div class=row>
    <div class=column >
    <ul>
      <li/> Induced over seconds, persistance over >10 hours
      <li/> Many mechanisms: Change in number of Receptors, Release Probability, ...
    </ul>
    </div>
    <div class=column >
    <ul>
      <li/> Induced over fractions of a second
      <li/> Recovery over seconds
      <li/> Change in probability of vesicle release, ...
    </ul>
    </div>
  </div class=row>
  <p class=ref>Feldman09_syna-mech<p>
  <p class=ref>Slide modified from Gerstner <i>et al.</i> 2015</p>
</textarea></section>

<section data-markdown><textarea data-template>
  <h2>Hebban Learning</h2>
  <div class=row>
  <div class=column>
  <img src="images/gerstner_hebb_rule.png"  class=small />
  </div>
  <div class=column>
  <img src="images/hebb_assemblies.jpg" />
  </div>
  </div>
  When an axon of cell $j$ repeatedly or persistently takes part in activating cell $i$, then $j$'s efficiency as one of the cells activating $i$ is increased
  <p class=ref>Hebb49_orga-beha</p>

$$
  \frac{\mathrm{d}}{\mathrm{d} t} w_{ij}(t) = \eta \nu_i \nu_j
$$

  <div class=row>
  <div class=column>
  <ul>
    <li/> Plasticity rule operating on local information
    <li/> Captures correlations in activity
    <li/> Unsupervised
  </ul>
  </div>
  <div class=column>
  <blockquote>''Neurons that fire together wire together''</blockquote>
  </div>
  </div>
</textarea></section>

<section data-markdown><textarea data-template>
  <h2>Hebb's Cell Assembly</h2>
  <img src="images/bear-24-05.png" />
</textarea></section>

<section data-markdown><textarea data-template>
  <h2>Generalized Hebbian Learning</h2>
  Generalized Hebbian learning: Introduce dependence on pre-synaptic and post-synaptic activities, and the weight itself:

$$
\begin{split}
  \frac{\mathrm{d}}{\mathrm{d} t} w_{ij}(t) &= F(w_{ij}, \nu_i, \nu_j)\\\\
  \frac{\mathrm{d}}{\mathrm{d} t} w_{ij}(t) &= a_0(w_{ij}) + a_1^{pre}(w_{ij})\nu_j +  a_1^{post}(w_{ij})\nu_i + a_2(w_{ij})\nu_i \nu_j + \dots \\\\
\end{split}
$$

  <center>
  <table>
    <tr>
      <td>Pre (Index j)</td> 
      <td>On</td> 
      <td>Off</td>
      <td>On</td>
      <td>Off</td>
    </tr>
    <tr>
      <td>Post (Index i) </td>
      <td>On</td> 
      <td>On</td> 
      <td>Off</td> 
      <td>Off</td>
    </tr>
    <tr class=fragment >
      <td>$\frac{\mathrm{d}}{\mathrm{d} t} w_{ij}(t) \propto \nu_i \nu_j$</td><td>+   </td><td> 0 </td><td> 0 </td><td> 0 </td>

    </tr>
    <tr class=fragment >
      <td>$\frac{\mathrm{d}}{\mathrm{d} t} w_{ij}(t) \propto \nu_i \nu_j - c$</td><td>+   </td><td> - </td><td> - </td><td> - </td>
    </tr>
    <tr class=fragment >
      <td> $\frac{\mathrm{d}}{\mathrm{d} t} w_{ij}(t) \propto (\nu_i - c)  \nu_j$ </td><td>(+) </td><td> 0 </td><td> - </td><td> 0 </td>
    </tr>
    <tr class=fragment >
      <td>$\frac{\mathrm{d}}{\mathrm{d} t} w_{ij}(t) \propto (\nu_i - \langle \nu_i \rangle)$</td><td>+   </td><td> - </td><td> - </td><td> + </td>
    </tr>
  </table>
  </center>
  <p class=ref>Gerstner_Kistler02_spik-neur</p>
</textarea></section>



<section data-markdown><textarea data-template>
  <h2>Modulated Hebb rule</h2>
  <b>Modulated Hebb rule: Neuromodulators + Hebbian Learning</b>
$$
\begin{split}
  \frac{\mathrm{d}}{\mathrm{d} t} w_{ij}(t) &= F(w_{ij}, \nu_i, \nu_j, mod(t))\\\\
\end{split}
$$

  Example causes of neuromodulation can be rewards, error, attention, novelty.

  <b>Examples:</b>
  <ul>
    <li class=fragment> Reinforcement learning:

$$
\begin{split}
  \frac{\mathrm{d}}{\mathrm{d} t} w_{ij}(t) \propto Reward(t) \nu_i \nu_j\\
\end{split}
$$

  <p class=ref>Florian07_rein-lear</p>
    </li>

    <li class=fragment > Supervised Learning:
$$
\begin{split}
  \frac{\mathrm{d}}{\mathrm{d} t} w_{ij}(t) &= Error_i(t) a_1^{pre}\nu_j\\
\end{split}
$$
    </li>
  </ul>
  
  <p class=pl> Some modulated synaptic plasticity rules are recently called three factor rules. </p>
</textarea></section>

<section data-markdown><textarea data-template>
  <h2>Spike-Timing Dependent Plasticity</h2>
  <img src="images/bi_poo_scholarpedia.jpeg"/>
      <p class=ref>Bi_Poo98_syna-modi</p>
      <p class=ref>Jesper Sjostrom and Wulfram Gerstner (2010), Scholarpedia, 5(2):1362.</p>

      <ul>
      <li>$W$: Learning Window</li>
      <li>$t_i^n$: $n$th spike time of post-synaptic neuron $i$</li>
      <li>$t_j^f$: $f$th spike time of pre-synaptic neuron $i$</li>
      </ul>
</textarea></section>

<section data-markdown><textarea data-template>
    <h2>Spike-Timing Dependent Plasticity (STDP)</h2>
    <img src="images/bi_poo_scholarpedia.jpeg" />
    <p class=ref>Gerstner_Kistler02_spik-neur</p>
     Spike-Time Dependent Plasticity Rule: 
$$
\Delta w_j = \sum_{f=1}^N \sum_{n=1}^N W(t_i^n - t_j^f)
$$


      <p class=ref>Jesper Sjostrom and Wulfram Gerstner (2010), Scholarpedia, 5(2):1362.</p>
</textarea></section>

<section data-markdown><textarea data-template>
  <h2>The Concept of Locality</h2>
  For computation to occur on a physical substrate, information much be spatially and temporally local.
  <img src=images/local_information.png />
  <p class=ref>Neftci_etal19_surrgrad</p>
</textarea></section>

<section data-markdown><textarea data-template>
  <h2>Back-Propagating Action Potentials</h2>
  <b>STDP requires synapses to sense post-synaptic neuron spike times</b> 
  <div class=row>
    <div class=column > Long-term potentiation is regulated by coincidence of postsynaptic APs and EPSPs
      <img src=images/Markram97_Fig_1.png ></img>
  <p class=ref>Markram_etal97</p>
    </div>
    <div class=column > Somadendritic Backpropagation of Action Potentials in Cortical Pyramidal Cells
      <img src=images/Buzsaki_Kandel98_Fig2.png />
      <p class=ref>Buzsaki and Kandel, J. Neurophysiol. 79: 1587--1591, 1998</p>
    </div>

  </div class=row>
</textarea></section>

<section data-markdown><textarea data-template>
    <h2>Spike-Timing Dependent Plasticity (STDP) Implementation</h2>
     Online implementation of the Spike-Time Dependent Plasticity Rule using pre-synaptic trace $P_j$ and post-synaptic trace $P_i$: 
$$
 \begin{split}
   \tau_+{\mathrm{d} \over \mathrm{d}t}P_j &=    -P_j  + S^{pre}_j\\\\
   \tau_-{\mathrm{d}\over   \mathrm{d}t} P_i &=    -P_i + S^{post}_i\\\\
   {\mathrm{d} \over \mathrm{d}t}w_{ij} &=    a_+ P_j(t) S^{post}_i + a_- P_i(t) S^{pre}_j
 \end{split}
$$      
     <ul>
      <li>$\delta(t)$: Delta Dirac function (= spike at time $t$)</li>
      <li>$a_+$: Amplitude of LTP &nbsp; $a_-$: Amplitude of LTD</li>
      <li>$\tau_+$: Temporal window of LTP</li>
      <li>$\tau_-$: Temporal window of LTD</li>
     </ul>
</textarea></section>


<section data-markdown><textarea data-template>
  <h2>STDP as Spike-Based Hebbian Learning</h2>
  <img src=images/learning_window_Gerstner_Kistler02.png />
  If the pre- and post-synaptic neuron spike times are independent:
$$
 \langle \frac{\mathrm{d}}{\mathrm{d}t} w_{ij} \rangle \cong  \nu_i \nu_j \underbrace{\int W(s) \mathrm{d}s}_{\text{Area under learning window}}
$$
</textarea></section>

<section data-markdown><textarea data-template>
  <h2>STDP as Spike-Based Generalized Hebbian Learning</h2>
  <p>A more general spike-time dependent plasticity rule</p> 
$$
\frac{\mathrm{d}}{\mathrm{d}t} w_j =  a_0(w_{ij}) +  a_1^{pre}(w_{ij}) S^{pre}_j + a_1^{post} (w_{ij}) S^{post}_i +  a_+ P_j(t) S^{post}_i + a_- P_i(t) S^{pre}_j 
$$

<p> If spike times are independent, the temporal average of generalized STDP implements the generalized Hebb rule:</p>
$$
\langle \frac{\mathrm{d}}{\mathrm{d}t} w_{ij} \rangle \cong a_0(w_{ij}) + a_1^{pre}(w_{ij}) \nu_j + a_1^{post}(w_{ij})\nu_i + \nu_i \nu_j \int W(s) \mathrm{d}s 
$$

</textarea></section>


<section data-markdown><textarea data-template>
  <h2>Notes about STDP</h2>
  <ul>
    <li/> Rate-based models are consistent with STDP
    <li/> Spike-time dependence depends on synapse location wrt soma
    <li/> The exponential fit of STDP is for computational convenience
    <li/> Update in original model is relative
      <img src=images/Bi_Poo_Fig_7.png />
    <li/> STDP is not derived from computational requirements
  </ul>
  
  <p class=pl >STDP is a measurement, not an accurate mechanistic model!</p>
</textarea></section>


<section data-markdown><textarea data-template>
  <h2>Normative Models of Synaptic Plasticity</h2>
  <ul>
    <li/> Rather than building synaptic plasticity from the bottom-up (as in STDP) Normative model strat with a mathematical model, and make hypotheses about how these could be implemented in synapses.
    <li/> Machine learning is a common source of inspiration for normative modeling
  </ul>
</textarea></section>

<section data-markdown><textarea data-template>
<h2> Models of RNNs in Neuroscience: Surrogate Gradient Learning</h2>
<div class=row>
<img src="images/sgdecolle.png">
</div>
<p class=ref>Neftci, Mostafa, Zenke, 2019</p>
<ul>
  <li/> Models biological neurons as artificial recurrent neural networks and uses approximate gradient-based learning
  <li/> Recurrent connections are trained with partial knowledge of the history
</ul>
</textarea></section>

<section data-markdown><textarea data-template>
<h2> "Biological" neuron model: The Leaky Integrate and Fire Neuron. </h2>                                                     

<div>
$$
\begin{align*}
U^{t+1} & = \beta  U^t + (1-\beta) W S^t_{in} - S^t \tag{Membrane Potential}\\
S^t &= \Theta(U^t-1) \tag{Spike & Reset} \\
\beta & = \exp(\frac{t}{\tau_{mem}})\\
\end{align*}
$$
</div>

          <img src="images/leaky_if.png" />
        </textarea></section>


        <section data-markdown><textarea data-template>
        <h2> Surrogate Gradient Learning </h2>                                                     

        <img src="images/surr_grad_1.svg" class=stretch />
        </textarea></section>

        <section data-markdown><textarea data-template>
        <h2> Surrogate Gradient Learning </h2>                                                     

        <img src="images/surr_grad_2.svg" class=stretch />
        </textarea></section>

        <section data-markdown><textarea data-template>
        <h2> Surrogate Gradient Learning </h2>                                                     

        <img src="images/surr_grad_3.svg" class=stretch />
        </textarea></section>

        <section data-markdown><textarea data-template>
        <h2> Surrogate Gradient Learning </h2>                                                     

        <img src="images/surr_grad_4.svg" class=stretch />
        </textarea></section>

        <section data-markdown><textarea data-template>
        <h2> Surrogate Gradient Learning </h2>                                                     

        <img src="images/surr_grad_5.svg" class=stretch />
        </textarea></section>


        <section data-markdown><textarea data-template>
        <h2> Surrogate Gradient Learning </h2>                                                     

        <img src="images/sg_loss_cartoon.svg" class=large />
        <ul>
          <li/> With surrogate gradients, we can train any biological neuron dynamics using gradient backpropagation 
          <li/> By approximating the temporal credit assignment problem, the gradient descent update is compatible with synaptic plasticity dynamics
        </ul>

        [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1-0S0iL0CVh72tXdBrZglZ5RPilcXSwik?usp=sharing)
        </textarea></section>

        <section data-markdown><textarea data-template>
        <h2> Why use recurrent neural networks</h2>                                                     
        <ul>
          <li> 
            <div><div class=column >Few recurrent connections in shallow neural networks can give them similar power to deep neural networks</div><div class=column >
            <img src="images/cornet-brainscore.png" class=large />
            <p class=ref>Schrimpf et al. 2019</p></div></div></li>
          <li class=fragment >Recurrent neural networks are Turing complete, <em> i.e.</em> they can theoretically emulated any computable algorithm</li>
          <li class=fragment >We may not have found the right way to train recurrent neural networks yet <p class=ref>Miller and Hardt, International Conference on Learning Representations, 2019</p> </li> 
            
          <li class=fragment >The real world is continuous-time, physical computing systems (<em>e.g.</em> biological neurons) operate under real-time constraints.</li> 
        </ul>
        </textarea></section>
</section>
