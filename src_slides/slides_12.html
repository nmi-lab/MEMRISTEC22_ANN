        <section data-markdown data-vertical-align-top data-background-color=#B2BA67><textarea data-template>
            <h1> Memristor Models in PyTorch<br/> </h1>
            <ul>
              <li/> Slides derived from <a href='https://aihwkit.readthedocs.io/'>AIHWKIT Documentation</a>
        </textarea></section>

      <section data-markdown><textarea data-template>
      <h2> In-Memory Computing</h2>                                                     
      <img src="img/analog_chip_description.png" />
      <ul>
        <li class="fragment"/> Goal: Multiple memristive crossbar arrays that communicate with each other
        <li /> Typical scenario: you would like to test performance/power/latency on AI benchmarks, but do not have large enough arrays at you disposal 
          <li class="fragment"/> <b>Offchip Training:</b> weights are typically trained using a conventional process and transferred (easy)
        <li class="fragment"/>   <b>Onchip Training: </b> weights are trained on the device (hard/open problem)
      </ul>
      </textarea></section>

      <section data-markdown><textarea data-template>
      <h2> Memristor Typees and Non-Idealities</h2>                                                     
      <img src="img/analog_ai_hw.png" class="large"/>
      <ul>
        <li class="fragment"/> Different device technologies have different non-idealities
        <ul>
          <li/> Asymmetry
          <li/> Cycle-to-cycle noise
          <li/> Device-to-device noise
          <li/> Read and/or write non-linearity
          <li/> Drift, endurance, precision/ dynamic range ...
      </ul>
      </textarea></section>

      <section data-markdown><textarea data-template>
      <h2> Memristor Non-Idealities</h2>                                                     
      <div class=row>
      <div class=column>
      <p>Aymmetric non-linearity</p>
      <img src="img/Agarwal_etal16_asymmetry_memristor_model1.png" class="medium"/>
      </div>
      <div class=column>

      <p>Effects of aymmetric non-linearity</p>
      <img src="img/Agarwal_etal16_asymmetric_memristor_figure2.png" class="large" />
      </div>
      </div>
      <p class=ref>[Agarwal_etal16_resimemo]</p>
      <div class=row>
      <div class=column>
        <p>Cycle to cycle non-linearity</p>
      <img src="img/analog_non_idealities.png" class="medium"/>
      </div>
      <div class=column>
        <p>Device to Device non-linearity</p>
      <img src="img/" class="medium"/>
      </div>
      </div>
      <ul>
        <li class="fragment"/> It is necessary to take these non-idealities into account during training
      </ul>
      </textarea></section>

      <section data-markdown><textarea data-template>
      <h2> Mapping conductances to weights</h2>                                                     
      <img src="img/toolkit_quantization.png" />
      <ul>
        <li class="fragment"/> 
      </ul>
      </textarea></section>

      <section data-markdown><textarea data-template>
      <h2> A Simple Memristor Model </h2>                                                     

      <p> Update Model:
      $$ 
      \begin{eqnarray*}
      w_{ij}  &\leftarrow&  w_{ij} - \Delta w_{ij}^d(\gamma_{ij}^d\;w_{ij}
      + 1 + \sigma_\text{c-to-c}\,\xi)\\
      w_{ij}  &\leftarrow& \text{clip}(w_{ij},b^\text{min}_{ij},b^\text{max}_{ij})
      \end{eqnarray*}  
      $$
      </p>

      <ul>
        <li/> $\sigma_\text{c-to-c}$ variance of cycle-to-cycle noise
        <li/> $\gamma_{ij}^d$ crosspoint-dependent Slope parameter  initalized as 
          $$\begin{eqnarray*}
\gamma_{ij}^+ &=& - |\gamma^+ + \gamma_\text{d-to-d}^+ \xi|/b^\text{max}_{ij}\\
\gamma_{ij}^- &=& - |\gamma^- + \gamma_\text{d-to-d}^- \xi|/b^\text{min}_{ij}
\end{eqnarray*}$$
<li/> b_{ij} crosspoint-dependent boundaries
      </ul>
      </textarea></section>

      <section data-markdown><textarea data-template>
      <h2> Offline learning: Custom Dynamics in pure PyTorch</h2>                                                     
      <ul>
        <li/> Here, we assume that the learning is done offline, in a way that is consistent with the device
        <li/> We implement read dynamics according to the device
        <li/> We train using exact updates. This is called hardware-aware training
      </ul>
      </textarea></section>

      <section data-markdown><textarea data-template>
      <h2> Online learning: Custom Updates in pure PyTorch</h2>                                                     
      <ul>
        <li/> Here, we assume that the devices are updated online.
        <li/> Recall that updates $$w \leftarrow w -\eta \nabla_w \mathcal{L}$$ are applied at the optimizer.step(). 
        <li/> So we need to create a custom optimizer
        <li/> TBD: code for custom optimizer
      </ul>
      </textarea></section>

      <section data-markdown><textarea data-template>
      <h2> AI Hardware Kit</h2>                                                     
      <div class=row>
        <div class=column>
        <img src="img/aihwkit.png" />
        </div>
        <div class=column>
        <img src="img/reram_measurements.png" />
        </div>
      </div>
      <p class="ref">(Rasch et al. 2021)</p>
      <ul>
        <li class="fragment"/> aihwkit: Acceleration of training of crossbar arrays using PyTorch and GPUs.
        <li/> Programmable Functional simulator of forward and backward pass (for online training)
        <li/> Here we focus on online training, but aihwkit can also do offline training (hardware-aware training)
      </ul>
      </textarea></section>

      <section data-markdown><textarea data-template>
      <h2> Device configuration with aihwkit: RPUConfig</h2>                                                     
      <ul>
        <li class="fragment"/> 
        <li/> [TBD: Describe RPUConfig here with code block]
      </ul>
      </textarea></section>

      <section data-markdown><textarea data-template>
      <h2> Optimizer with aihwkit:AnalogSGD </h2>                                                     
      <ul>
        <li class="fragment"/> Hands-on tutorial aihwkit: 
        <li/> [TBD: Describe AnalogSGD here with code block]
        <li class="fragment"/> Hands-on tutorial aihwkit: 
      </ul>

      [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1WAAf5PEt5tgj4ah2vVm2gmA_nhYD8bVT?usp=sharing)

      </textarea></section>



 


 
